{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy pandas\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFlow:\\n\\n-- 11:45 pm --\\n\\nAssumptions For data creation:\\n1. Data consist of 1 brand \\n2. 500 SKU unique SKU in the brand. Across 10 categories\\n3. 2 years of data\\n4. Have 10_000 unique users\\n6. This is a conversion level data, not event level data\\n7. At a conversion level, datapoints available: timestamp, sku, sku_category, revenue_through_conversion\\n8. User level data: \\n   - Demographic: Single country, multiple states [A - F]\\n   - Gender, age\\n\\n\\nChurn Definition:\\n1. No purchase in the last 30 days -> We declare customer has churned\\n\\n–\\nTraining Process & training sample creation:\\n1. We have 2 years of data across 10_000 customer\\n2. We create training data for F.E. with churn label by:\\n    - start with 6 month of data for creating labels, customer with no purchase in next 30 days are marked as churned.\\n    - Now we keep increasing data for creating labels by one month (7, 8, 9, 10, and so on) until 1 year 10 months.\\n    - This way we would be able to generate multiple training samples with limited number of users\\n\\n-- 2:30 pm -- \\n\\n3. Train a boosing models -> predict churn probability\\n    - Would look at PR-AUC\\n\\nInference:\\n1. We inference of whole 2 years of data.\\n2. Define buckets for churn (>90, 90-70, 70-30, <30)\\n\\n-- 4:00 pm -- \\n\\nDasboard:\\n1. Make a streamlit dashboard, host it locally. \\n - Show summary\\n - each user level row (high rish customer)\\n - corresponding button to each row with trigger an action.\\n\\nVoice model to make a phone call:\\n- \\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Flow:\n",
    "\n",
    "-- 11:45 pm --\n",
    "\n",
    "Assumptions For data creation:\n",
    "1. Data consist of 1 brand \n",
    "2. 500 SKU unique SKU in the brand. Across 10 categories\n",
    "3. 2 years of data\n",
    "4. Have 10_000 unique users\n",
    "6. This is a conversion level data, not event level data\n",
    "7. At a conversion level, datapoints available: timestamp, sku, sku_category, revenue_through_conversion\n",
    "8. User level data: \n",
    "   - Demographic: Single country, multiple states [A - F]\n",
    "   - Gender, age\n",
    "\n",
    "\n",
    "Churn Definition:\n",
    "1. No purchase in the last 30 days -> We declare customer has churned\n",
    "\n",
    "–\n",
    "Training Process & training sample creation:\n",
    "1. We have 2 years of data across 10_000 customer\n",
    "2. We create training data for F.E. with churn label by:\n",
    "    - start with 6 month of data for creating labels, customer with no purchase in next 30 days are marked as churned.\n",
    "    - Now we keep increasing data for creating labels by one month (7, 8, 9, 10, and so on) until 1 year 10 months.\n",
    "    - This way we would be able to generate multiple training samples with limited number of users\n",
    "\n",
    "-- 2:30 pm -- \n",
    "\n",
    "3. Train a boosing models -> predict churn probability\n",
    "    - Would look at PR-AUC\n",
    "\n",
    "Inference:\n",
    "1. We inference of whole 2 years of data.\n",
    "2. Define buckets for churn (>90, 90-70, 70-30, <30)\n",
    "\n",
    "-- 4:00 pm -- \n",
    "\n",
    "Dasboard:\n",
    "1. Make a streamlit dashboard, host it locally. \n",
    " - Show summary\n",
    " - each user level row (high rish customer)\n",
    " - corresponding button to each row with trigger an action.\n",
    "\n",
    "Voice model to make a phone call:\n",
    "- \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total conversions to generate: 502359\n",
      "  user_id    country    state  gender  age\n",
      "0  user_1  country_x  state_8    male   39\n",
      "1  user_2  country_x  state_2  female   69\n",
      "2  user_3  country_x  state_1  female   24\n",
      "3  user_4  country_x  state_2  female   31\n",
      "4  user_5  country_x  state_5    male   53\n",
      "-----------------------------------------------------------------\n",
      "     user_id  timestamp brand      sku sku_category  \\\n",
      "0  user_3864 2023-01-01   xyz  sku_159        cat_6   \n",
      "1  user_3217 2023-01-01   xyz  sku_403        cat_2   \n",
      "2  user_1681 2023-01-01   xyz   sku_91        cat_6   \n",
      "3   user_300 2023-01-01   xyz  sku_189        cat_2   \n",
      "4  user_2205 2023-01-01   xyz  sku_418        cat_6   \n",
      "\n",
      "   revenue_through_conversion  \n",
      "0                       48.02  \n",
      "1                       15.22  \n",
      "2                       37.94  \n",
      "3                       24.25  \n",
      "4                       18.44  \n",
      "-----------------------------------------------------------------\n",
      "Users: 10000\n",
      "-----------------------------------------------------------------\n",
      "Conversions: 502359\n",
      "-----------------------------------------------------------------\n",
      "Category distribution (conversions):\n",
      "sku_category\n",
      "cat_2     0.401546\n",
      "cat_6     0.199843\n",
      "cat_5     0.099720\n",
      "cat_1     0.099534\n",
      "cat_7     0.079587\n",
      "cat_4     0.050102\n",
      "cat_3     0.049554\n",
      "cat_8     0.010112\n",
      "cat_10    0.005066\n",
      "cat_9     0.004937\n",
      "Name: proportion, dtype: float64\n",
      "-----------------------------------------------------------------\n",
      "Total Revenue Generated:\n",
      "18030957.79\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "brand = 'xyz'\n",
    "sku = [f'sku_{i}' for i in range(1, 501)]\n",
    "categories = [f'cat_{i}' for i in range(1, 11)]\n",
    "gender = ['male', 'female', 'others']\n",
    "country = ['country_x']\n",
    "states = [f'state_{i}' for i in range(1, 16)]\n",
    "users = [f'user_{i}' for i in range(1, 10_001)] \n",
    "\n",
    "# category to SKU proportion mapping \n",
    "sku_cat_prop_map = [0.1, 0.4, 0.05, 0.05, 0.1, 0.2, 0.08, 0.01, 0.005, 0.005]\n",
    "sku_cat_prop_map = np.array(sku_cat_prop_map) / np.sum(sku_cat_prop_map)  \n",
    "\n",
    "# gender proportion mapping (male 40%, female 59%, others 1%)\n",
    "gender_user_prop_map = np.array([0.4, 0.59, 0.01])\n",
    "\n",
    "# time window (2 years)\n",
    "start_date = pd.Timestamp(\"2023-01-01\")\n",
    "end_date = pd.Timestamp(\"2024-12-31\")\n",
    "n_days = (end_date - start_date).days + 1\n",
    "\n",
    "#User level Data\n",
    "n_users = len(users)\n",
    "user_genders = np.random.choice(gender, size = n_users, p = gender_user_prop_map)\n",
    "user_states = np.random.choice(states, size = n_users)\n",
    "user_countries = np.random.choice(country, size = n_users, p = [1])\n",
    "user_ages = np.random.randint(18, 71, size=n_users)\n",
    "\n",
    "df_users = pd.DataFrame({\"user_id\": users, \"country\": user_countries, \"state\": user_states, \"gender\": user_genders, \"age\": user_ages})\n",
    "\n",
    "def generate_random_mappings_bw_two_entity(entity1, entity2, map_dict):\n",
    "\n",
    "    users_accumulated = entity1\n",
    "    sku_cat_prop_map_dict = {}\n",
    "\n",
    "    for i, cat_item in enumerate(entity2):\n",
    "        sku_for_cat = np.random.choice(users_accumulated, math.floor((map_dict[i])*len(entity1)))\n",
    "        sku_cat_prop_map_dict[cat_item] = sku_for_cat\n",
    "        users_accumulated = [x for x in users_accumulated if x not in sku_for_cat]\n",
    "    \n",
    "    return sku_cat_prop_map_dict    \n",
    "\n",
    "cat_to_skus = generate_random_mappings_bw_two_entity(sku, categories, sku_cat_prop_map)\n",
    "\n",
    "# conversion Level Data(assuming: for each user, number of purchases between 1 and 100) -> This is non-deterministic\n",
    "purchases_per_user = np.random.randint(1, 101, size=n_users)\n",
    "total_conversions = purchases_per_user.sum()\n",
    "print(f\"Total conversions to generate: {total_conversions}\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "for user_id, num_purchases in zip(users, purchases_per_user):\n",
    "    # Sample timestamps uniformly over 2 years\n",
    "    day_offsets = np.random.randint(0, n_days, size=num_purchases)\n",
    "    timestamps = start_date + pd.to_timedelta(day_offsets, unit=\"D\")\n",
    "\n",
    "    purchase_categories = np.random.choice(categories, size=num_purchases, p=sku_cat_prop_map)\n",
    "\n",
    "    # for each cat pick a SKU\n",
    "    purchase_skus = [np.random.choice(cat_to_skus[cat]) for cat in purchase_categories]\n",
    "\n",
    "    # Synthetic revenue through conversion\n",
    "    raw_revenue = np.random.lognormal(mean=3.4, sigma=0.6, size=num_purchases)\n",
    "    revenue = np.round(np.clip(raw_revenue, 1, None), 2)\n",
    "\n",
    "    for ts, cat, s, rev in zip(timestamps, purchase_categories, purchase_skus, revenue):\n",
    "        rows.append({\"user_id\": user_id, \"timestamp\": ts, \"brand\": brand, \"sku\": s, \"sku_category\": cat, \"revenue_through_conversion\": rev})\n",
    "\n",
    "df_conversions = pd.DataFrame(rows)\n",
    "df_conversions = df_conversions.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "print(df_users.head())\n",
    "print('-----------------------------------------------------------------')\n",
    "print(df_conversions.head())\n",
    "print('-----------------------------------------------------------------')\n",
    "print(\"Users:\", df_users[\"user_id\"].nunique())\n",
    "print('-----------------------------------------------------------------')\n",
    "print(\"Conversions:\", len(df_conversions))\n",
    "print('-----------------------------------------------------------------')\n",
    "print(\"Category distribution (conversions):\")\n",
    "print(df_conversions[\"sku_category\"].value_counts(normalize=True))\n",
    "print('-----------------------------------------------------------------')\n",
    "print(\"Total Revenue Generated:\")\n",
    "print(df_conversions[\"revenue_through_conversion\"].sum())\n",
    "print('-----------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id    0\n",
       "country    0\n",
       "state      0\n",
       "gender     0\n",
       "age        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                       0\n",
       "timestamp                     0\n",
       "brand                         0\n",
       "sku                           0\n",
       "sku_category                  0\n",
       "revenue_through_conversion    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conversions.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating training data (Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conversions = df_conversions.merge(df_users, how = 'right', on = 'user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                       0\n",
       "timestamp                     0\n",
       "brand                         0\n",
       "sku                           0\n",
       "sku_category                  0\n",
       "revenue_through_conversion    0\n",
       "country                       0\n",
       "state                         0\n",
       "gender                        0\n",
       "age                           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conversions.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>brand</th>\n",
       "      <th>sku</th>\n",
       "      <th>sku_category</th>\n",
       "      <th>revenue_through_conversion</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_1</td>\n",
       "      <td>2023-01-30</td>\n",
       "      <td>xyz</td>\n",
       "      <td>sku_495</td>\n",
       "      <td>cat_2</td>\n",
       "      <td>17.19</td>\n",
       "      <td>country_x</td>\n",
       "      <td>state_8</td>\n",
       "      <td>male</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_1</td>\n",
       "      <td>2023-03-24</td>\n",
       "      <td>xyz</td>\n",
       "      <td>sku_282</td>\n",
       "      <td>cat_2</td>\n",
       "      <td>36.54</td>\n",
       "      <td>country_x</td>\n",
       "      <td>state_8</td>\n",
       "      <td>male</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_1</td>\n",
       "      <td>2023-05-19</td>\n",
       "      <td>xyz</td>\n",
       "      <td>sku_219</td>\n",
       "      <td>cat_2</td>\n",
       "      <td>16.30</td>\n",
       "      <td>country_x</td>\n",
       "      <td>state_8</td>\n",
       "      <td>male</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_1</td>\n",
       "      <td>2023-08-16</td>\n",
       "      <td>xyz</td>\n",
       "      <td>sku_315</td>\n",
       "      <td>cat_2</td>\n",
       "      <td>52.18</td>\n",
       "      <td>country_x</td>\n",
       "      <td>state_8</td>\n",
       "      <td>male</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_1</td>\n",
       "      <td>2023-12-17</td>\n",
       "      <td>xyz</td>\n",
       "      <td>sku_18</td>\n",
       "      <td>cat_2</td>\n",
       "      <td>39.82</td>\n",
       "      <td>country_x</td>\n",
       "      <td>state_8</td>\n",
       "      <td>male</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id  timestamp brand      sku sku_category  revenue_through_conversion  \\\n",
       "0  user_1 2023-01-30   xyz  sku_495        cat_2                       17.19   \n",
       "1  user_1 2023-03-24   xyz  sku_282        cat_2                       36.54   \n",
       "2  user_1 2023-05-19   xyz  sku_219        cat_2                       16.30   \n",
       "3  user_1 2023-08-16   xyz  sku_315        cat_2                       52.18   \n",
       "4  user_1 2023-12-17   xyz   sku_18        cat_2                       39.82   \n",
       "\n",
       "     country    state gender  age  \n",
       "0  country_x  state_8   male   39  \n",
       "1  country_x  state_8   male   39  \n",
       "2  country_x  state_8   male   39  \n",
       "3  country_x  state_8   male   39  \n",
       "4  country_x  state_8   male   39  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_conversions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Defined process\n",
    "Process of creating training samples:\n",
    "1. We have 2 years of data at user, timestamp level -> each conversion for a user is a different row.\n",
    "2. I to create training sample for churn prediction in a way that \n",
    " - Start by looking at first x months of data, create features (unique_categories, unique_sku, count_conversions, sum_revenue, recvency, frequency, etc)\n",
    " - look at next 2 months -> if no conversion for a user, mark that user a churned\n",
    "3. Keep increaisng x by 1-2 months and keep incrementing the trainns sample.\n",
    "\n",
    "label = churn_in_next_2_months\n",
    "\n",
    "df_conversions = df_conversions.merge(df_users, on=\"user_id\", how=\"left\")\n",
    "\n",
    "#training sample creation for churn prediction\n",
    "#these params can be changed\n",
    "lookback_months = 6         \n",
    "forward_window_months = 2   \n",
    "step_months = 1             \n",
    "\n",
    "df_conversions['timestamp'] = pd.to_datetime(df_conversions['timestamp'])\n",
    "\n",
    "def compute_user_features(df_window, window_end):\n",
    "    if df_window.empty:\n",
    "        empty_df = pd.DataFrame(columns=[\"user_id\", \"unique_categories\", \"unique_skus\", \"count_conversions\", \"sum_revenue\", \"recency\", \"frequency\"])\n",
    "        return empty_df\n",
    "\n",
    "    user_grouped = df_window.groupby(\"user_id\")\n",
    "    features_created = user_grouped.agg(unique_categories=('sku_category', 'nunique'), unique_skus=('sku', 'nunique'),\n",
    "        count_conversions=('user_id', 'count'), sum_revenue=('revenue_through_conversion', 'sum'),\n",
    "        recency=('timestamp', lambda x: (window_end - x.max()).days),\n",
    "        frequency=('timestamp', lambda x: x.count() / ((x.max() - x.min()).days + 1)if x.count() > 1 else 0.0)).reset_index()\n",
    "\n",
    "    return features_created\n",
    "\n",
    "\n",
    "def compute_churn_labels(df_future, all_users):\n",
    "\n",
    "    active_users = df_future['user_id'].unique()\n",
    "    label_df = pd.DataFrame({\"user_id\": all_users})\n",
    "    label_df[\"churn_label\"] = (~label_df[\"user_id\"].isin(active_users)).astype(int)\n",
    "    return label_df\n",
    "\n",
    "\n",
    "def generate_training_samples(df_conv, df_users, lookback_months, forward_months, step_months):\n",
    "    df = df_conv.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "    start_date = df['timestamp'].min()\n",
    "    end_date = df['timestamp'].max()\n",
    "\n",
    "    training_rows = []\n",
    "    current_start = start_date\n",
    "\n",
    "    while True:\n",
    "        window_start = current_start\n",
    "        window_end = window_start + pd.DateOffset(months=lookback_months)\n",
    "        future_end = window_end + pd.DateOffset(months=forward_months)\n",
    "\n",
    "        # stop if we can't fit a full forward window\n",
    "        if future_end > end_date:\n",
    "            break\n",
    "\n",
    "        # data in lookback window\n",
    "        df_window = df[(df['timestamp'] >= window_start) &\n",
    "                       (df['timestamp'] < window_end)].copy()\n",
    "\n",
    "        # data in forward window\n",
    "        df_future = df[(df['timestamp'] >= window_end) &\n",
    "                       (df['timestamp'] < future_end)].copy()\n",
    "\n",
    "        if df_window.empty:\n",
    "            current_start += pd.DateOffset(months=step_months)\n",
    "            continue\n",
    "\n",
    "        # features for users with activity in lookback window\n",
    "        feats = compute_user_features(df_window, window_end)\n",
    "\n",
    "        # churn labels for all users (we will only keep those with feats)\n",
    "        labels = compute_churn_labels(df_future, df_users['user_id'])\n",
    "\n",
    "        # merge features + static user attributes + churn label\n",
    "        merged = (feats\n",
    "                  .merge(df_users, on=\"user_id\", how=\"left\")\n",
    "                  .merge(labels, on=\"user_id\", how=\"left\"))\n",
    "\n",
    "        merged[\"feature_window_start\"] = window_start\n",
    "        merged[\"feature_window_end\"] = window_end\n",
    "        merged[\"label_window_end\"] = future_end\n",
    "\n",
    "        training_rows.append(merged)\n",
    "\n",
    "        # slide window\n",
    "        current_start += pd.DateOffset(months=step_months)\n",
    "\n",
    "    if not training_rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    training_df = pd.concat(training_rows, ignore_index=True)\n",
    "    return training_df\n",
    "\n",
    "#final training data\n",
    "training_df = generate_training_samples(\n",
    "    df_conversions,\n",
    "    df_users,\n",
    "    lookback_months=lookback_months,\n",
    "    forward_months=forward_window_months,\n",
    "    step_months=step_months\n",
    ")\n",
    "\n",
    "print(\"Training samples shape:\", training_df.shape)\n",
    "print(training_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
